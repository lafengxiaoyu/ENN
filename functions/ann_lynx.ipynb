{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.api import qqplot\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout\n",
    "from sklearn import preprocessing\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lynx.csv', index_col = ['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:100]\n",
    "test = df[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 7)                 56        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 102\n",
      "Trainable params: 102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(7, input_dim=7, activation=\"tanh\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, activation=\"tanh\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "window_size = 7\n",
    "def make_model(window_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(7, input_dim=window_size, init=\"uniform\",\n",
    "    activation=\"tanh\"))\n",
    "    model.add(Dense(5, init=\"uniform\", activation=\"tanh\"))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_model(7)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train = np.array(train).reshape(-1,1)\n",
    "\n",
    "train_scaled = min_max_scaler.fit_transform(train) \n",
    "\n",
    "train_X,train_Y = [],[]\n",
    "for i in range(0 , len(train_scaled) - window_size):\n",
    "    train_X.append(train_scaled[i:i+window_size])\n",
    "    train_Y.append(train_scaled[i+window_size])\n",
    "\n",
    "new_train_X,new_train_Y = [],[]\n",
    "for i in train_X:\n",
    "    new_train_X.append(i.reshape(-1))\n",
    "for i in train_Y:\n",
    "    new_train_Y.append(i.reshape(-1))\n",
    "new_train_X = np.array(new_train_X)\n",
    "new_train_Y = np.array(new_train_Y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 88 samples, validate on 5 samples\n",
      "Epoch 1/2000\n",
      "88/88 [==============================] - 4s 42ms/step - loss: 0.1056 - val_loss: 0.0596\n",
      "Epoch 2/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.1041 - val_loss: 0.0585\n",
      "Epoch 3/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.1024 - val_loss: 0.0573\n",
      "Epoch 4/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.1006 - val_loss: 0.0561\n",
      "Epoch 5/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0988 - val_loss: 0.0550\n",
      "Epoch 6/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0971 - val_loss: 0.0540\n",
      "Epoch 7/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0953 - val_loss: 0.0529\n",
      "Epoch 8/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0936 - val_loss: 0.0519\n",
      "Epoch 9/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0919 - val_loss: 0.0510\n",
      "Epoch 10/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0903 - val_loss: 0.0500\n",
      "Epoch 11/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0886 - val_loss: 0.0491\n",
      "Epoch 12/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0870 - val_loss: 0.0483\n",
      "Epoch 13/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0854 - val_loss: 0.0475\n",
      "Epoch 14/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0839 - val_loss: 0.0467\n",
      "Epoch 15/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0823 - val_loss: 0.0460\n",
      "Epoch 16/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0808 - val_loss: 0.0453\n",
      "Epoch 17/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0794 - val_loss: 0.0447\n",
      "Epoch 18/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0779 - val_loss: 0.0441\n",
      "Epoch 19/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0765 - val_loss: 0.0435\n",
      "Epoch 20/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0751 - val_loss: 0.0430\n",
      "Epoch 21/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0738 - val_loss: 0.0426\n",
      "Epoch 22/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0724 - val_loss: 0.0422\n",
      "Epoch 23/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0711 - val_loss: 0.0418\n",
      "Epoch 24/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0699 - val_loss: 0.0415\n",
      "Epoch 25/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0687 - val_loss: 0.0412\n",
      "Epoch 26/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0675 - val_loss: 0.0410\n",
      "Epoch 27/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0664 - val_loss: 0.0409\n",
      "Epoch 28/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0653 - val_loss: 0.0408\n",
      "Epoch 29/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0642 - val_loss: 0.0408\n",
      "Epoch 30/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0632 - val_loss: 0.0408\n",
      "Epoch 31/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0622 - val_loss: 0.0408\n",
      "Epoch 32/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0613 - val_loss: 0.0410\n",
      "Epoch 33/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0604 - val_loss: 0.0411\n",
      "Epoch 34/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0595 - val_loss: 0.0413\n",
      "Epoch 35/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0587 - val_loss: 0.0415\n",
      "Epoch 36/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0580 - val_loss: 0.0418\n",
      "Epoch 37/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0572 - val_loss: 0.0421\n",
      "Epoch 38/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0565 - val_loss: 0.0423\n",
      "Epoch 39/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0559 - val_loss: 0.0426\n",
      "Epoch 40/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0553 - val_loss: 0.0430\n",
      "Epoch 41/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0547 - val_loss: 0.0433\n",
      "Epoch 42/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0541 - val_loss: 0.0436\n",
      "Epoch 43/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0536 - val_loss: 0.0438\n",
      "Epoch 44/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0531 - val_loss: 0.0441\n",
      "Epoch 45/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0526 - val_loss: 0.0443\n",
      "Epoch 46/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0521 - val_loss: 0.0445\n",
      "Epoch 47/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0517 - val_loss: 0.0447\n",
      "Epoch 48/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0512 - val_loss: 0.0448\n",
      "Epoch 49/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0508 - val_loss: 0.0448\n",
      "Epoch 50/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0504 - val_loss: 0.0448\n",
      "Epoch 51/2000\n",
      "88/88 [==============================] - 0s 39us/step - loss: 0.0500 - val_loss: 0.0448\n",
      "Epoch 52/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0495 - val_loss: 0.0446\n",
      "Epoch 53/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0491 - val_loss: 0.0445\n",
      "Epoch 54/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0487 - val_loss: 0.0442\n",
      "Epoch 55/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0482 - val_loss: 0.0439\n",
      "Epoch 56/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0478 - val_loss: 0.0435\n",
      "Epoch 57/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0473 - val_loss: 0.0431\n",
      "Epoch 58/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0469 - val_loss: 0.0426\n",
      "Epoch 59/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0464 - val_loss: 0.0421\n",
      "Epoch 60/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0459 - val_loss: 0.0415\n",
      "Epoch 61/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0454 - val_loss: 0.0408\n",
      "Epoch 62/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0449 - val_loss: 0.0401\n",
      "Epoch 63/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0444 - val_loss: 0.0394\n",
      "Epoch 64/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0439 - val_loss: 0.0387\n",
      "Epoch 65/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0433 - val_loss: 0.0379\n",
      "Epoch 66/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0428 - val_loss: 0.0371\n",
      "Epoch 67/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0423 - val_loss: 0.0363\n",
      "Epoch 68/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0418 - val_loss: 0.0355\n",
      "Epoch 69/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0412 - val_loss: 0.0347\n",
      "Epoch 70/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0407 - val_loss: 0.0339\n",
      "Epoch 71/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0402 - val_loss: 0.0331\n",
      "Epoch 72/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0397 - val_loss: 0.0323\n",
      "Epoch 73/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0392 - val_loss: 0.0315\n",
      "Epoch 74/2000\n",
      "88/88 [==============================] - 0s 40us/step - loss: 0.0387 - val_loss: 0.0308\n",
      "Epoch 75/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0382 - val_loss: 0.0301\n",
      "Epoch 76/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0377 - val_loss: 0.0294\n",
      "Epoch 77/2000\n",
      "88/88 [==============================] - 0s 35us/step - loss: 0.0372 - val_loss: 0.0287\n",
      "Epoch 78/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0368 - val_loss: 0.0281\n",
      "Epoch 79/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0363 - val_loss: 0.0275\n",
      "Epoch 80/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0359 - val_loss: 0.0269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0354 - val_loss: 0.0264\n",
      "Epoch 82/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0350 - val_loss: 0.0259\n",
      "Epoch 83/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0346 - val_loss: 0.0255\n",
      "Epoch 84/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0342 - val_loss: 0.0251\n",
      "Epoch 85/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0339 - val_loss: 0.0247\n",
      "Epoch 86/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0335 - val_loss: 0.0243\n",
      "Epoch 87/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0332 - val_loss: 0.0240\n",
      "Epoch 88/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0328 - val_loss: 0.0237\n",
      "Epoch 89/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0325 - val_loss: 0.0234\n",
      "Epoch 90/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0322 - val_loss: 0.0231\n",
      "Epoch 91/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0319 - val_loss: 0.0229\n",
      "Epoch 92/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0317 - val_loss: 0.0227\n",
      "Epoch 93/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0314 - val_loss: 0.0225\n",
      "Epoch 94/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0312 - val_loss: 0.0223\n",
      "Epoch 95/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0309 - val_loss: 0.0222\n",
      "Epoch 96/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0307 - val_loss: 0.0220\n",
      "Epoch 97/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0305 - val_loss: 0.0219\n",
      "Epoch 98/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0303 - val_loss: 0.0218\n",
      "Epoch 99/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0302 - val_loss: 0.0217\n",
      "Epoch 100/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0300 - val_loss: 0.0216\n",
      "Epoch 101/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0298 - val_loss: 0.0215\n",
      "Epoch 102/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0297 - val_loss: 0.0215\n",
      "Epoch 103/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0295 - val_loss: 0.0215\n",
      "Epoch 104/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0294 - val_loss: 0.0214\n",
      "Epoch 105/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0293 - val_loss: 0.0214\n",
      "Epoch 106/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0292 - val_loss: 0.0215\n",
      "Epoch 107/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0290 - val_loss: 0.0215\n",
      "Epoch 108/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0289 - val_loss: 0.0215\n",
      "Epoch 109/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0288 - val_loss: 0.0216\n",
      "Epoch 110/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0287 - val_loss: 0.0216\n",
      "Epoch 111/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0286 - val_loss: 0.0217\n",
      "Epoch 112/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0285 - val_loss: 0.0218\n",
      "Epoch 113/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0284 - val_loss: 0.0219\n",
      "Epoch 114/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0283 - val_loss: 0.0219\n",
      "Epoch 115/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0282 - val_loss: 0.0220\n",
      "Epoch 116/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0281 - val_loss: 0.0221\n",
      "Epoch 117/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0280 - val_loss: 0.0222\n",
      "Epoch 118/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0279 - val_loss: 0.0223\n",
      "Epoch 119/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0278 - val_loss: 0.0223\n",
      "Epoch 120/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0277 - val_loss: 0.0224\n",
      "Epoch 121/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0276 - val_loss: 0.0225\n",
      "Epoch 122/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0275 - val_loss: 0.0225\n",
      "Epoch 123/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0274 - val_loss: 0.0226\n",
      "Epoch 124/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0273 - val_loss: 0.0227\n",
      "Epoch 125/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0272 - val_loss: 0.0227\n",
      "Epoch 126/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0271 - val_loss: 0.0228\n",
      "Epoch 127/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0270 - val_loss: 0.0228\n",
      "Epoch 128/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0269 - val_loss: 0.0229\n",
      "Epoch 129/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0268 - val_loss: 0.0229\n",
      "Epoch 130/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0267 - val_loss: 0.0230\n",
      "Epoch 131/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0266 - val_loss: 0.0230\n",
      "Epoch 132/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0265 - val_loss: 0.0231\n",
      "Epoch 133/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0264 - val_loss: 0.0231\n",
      "Epoch 134/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0263 - val_loss: 0.0232\n",
      "Epoch 135/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0262 - val_loss: 0.0232\n",
      "Epoch 136/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0261 - val_loss: 0.0232\n",
      "Epoch 137/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0260 - val_loss: 0.0233\n",
      "Epoch 138/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0259 - val_loss: 0.0233\n",
      "Epoch 139/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0258 - val_loss: 0.0234\n",
      "Epoch 140/2000\n",
      "88/88 [==============================] - 0s 38us/step - loss: 0.0257 - val_loss: 0.0234\n",
      "Epoch 141/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0256 - val_loss: 0.0234\n",
      "Epoch 142/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0255 - val_loss: 0.0235\n",
      "Epoch 143/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0254 - val_loss: 0.0235\n",
      "Epoch 144/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0253 - val_loss: 0.0235\n",
      "Epoch 145/2000\n",
      "88/88 [==============================] - 0s 38us/step - loss: 0.0252 - val_loss: 0.0236\n",
      "Epoch 146/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0251 - val_loss: 0.0236\n",
      "Epoch 147/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0250 - val_loss: 0.0236\n",
      "Epoch 148/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0249 - val_loss: 0.0237\n",
      "Epoch 149/2000\n",
      "88/88 [==============================] - 0s 39us/step - loss: 0.0248 - val_loss: 0.0237\n",
      "Epoch 150/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0247 - val_loss: 0.0238\n",
      "Epoch 151/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0246 - val_loss: 0.0238\n",
      "Epoch 152/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0245 - val_loss: 0.0238\n",
      "Epoch 153/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0244 - val_loss: 0.0239\n",
      "Epoch 154/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0243 - val_loss: 0.0239\n",
      "Epoch 155/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0242 - val_loss: 0.0240\n",
      "Epoch 156/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0241 - val_loss: 0.0241\n",
      "Epoch 157/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0240 - val_loss: 0.0241\n",
      "Epoch 158/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0239 - val_loss: 0.0242\n",
      "Epoch 159/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0238 - val_loss: 0.0242\n",
      "Epoch 160/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0237 - val_loss: 0.0243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0236 - val_loss: 0.0244\n",
      "Epoch 162/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0235 - val_loss: 0.0244\n",
      "Epoch 163/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0234 - val_loss: 0.0245\n",
      "Epoch 164/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0234 - val_loss: 0.0246\n",
      "Epoch 165/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 166/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0232 - val_loss: 0.0247\n",
      "Epoch 167/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0231 - val_loss: 0.0248\n",
      "Epoch 168/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0230 - val_loss: 0.0248\n",
      "Epoch 169/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0229 - val_loss: 0.0249\n",
      "Epoch 170/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0228 - val_loss: 0.0250\n",
      "Epoch 171/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0227 - val_loss: 0.0251\n",
      "Epoch 172/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0226 - val_loss: 0.0252\n",
      "Epoch 173/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0225 - val_loss: 0.0252\n",
      "Epoch 174/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0224 - val_loss: 0.0253\n",
      "Epoch 175/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0223 - val_loss: 0.0254\n",
      "Epoch 176/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0222 - val_loss: 0.0255\n",
      "Epoch 177/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0221 - val_loss: 0.0256\n",
      "Epoch 178/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0220 - val_loss: 0.0257\n",
      "Epoch 179/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0219 - val_loss: 0.0257\n",
      "Epoch 180/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0218 - val_loss: 0.0258\n",
      "Epoch 181/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0217 - val_loss: 0.0259\n",
      "Epoch 182/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0217 - val_loss: 0.0260\n",
      "Epoch 183/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0216 - val_loss: 0.0261\n",
      "Epoch 184/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0215 - val_loss: 0.0262\n",
      "Epoch 185/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0214 - val_loss: 0.0263\n",
      "Epoch 186/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0213 - val_loss: 0.0264\n",
      "Epoch 187/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0212 - val_loss: 0.0265\n",
      "Epoch 188/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0211 - val_loss: 0.0266\n",
      "Epoch 189/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0210 - val_loss: 0.0267\n",
      "Epoch 190/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0209 - val_loss: 0.0268\n",
      "Epoch 191/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0209 - val_loss: 0.0269\n",
      "Epoch 192/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0208 - val_loss: 0.0270\n",
      "Epoch 193/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0207 - val_loss: 0.0271\n",
      "Epoch 194/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0206 - val_loss: 0.0272\n",
      "Epoch 195/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0205 - val_loss: 0.0273\n",
      "Epoch 196/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0204 - val_loss: 0.0274\n",
      "Epoch 197/2000\n",
      "88/88 [==============================] - 0s 60us/step - loss: 0.0204 - val_loss: 0.0275\n",
      "Epoch 198/2000\n",
      "88/88 [==============================] - 0s 60us/step - loss: 0.0203 - val_loss: 0.0276\n",
      "Epoch 199/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0202 - val_loss: 0.0277\n",
      "Epoch 200/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0201 - val_loss: 0.0278\n",
      "Epoch 201/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0200 - val_loss: 0.0279\n",
      "Epoch 202/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0200 - val_loss: 0.0280\n",
      "Epoch 203/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0199 - val_loss: 0.0281\n",
      "Epoch 204/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0198 - val_loss: 0.0282\n",
      "Epoch 205/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0197 - val_loss: 0.0283\n",
      "Epoch 206/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0197 - val_loss: 0.0284\n",
      "Epoch 207/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0196 - val_loss: 0.0286\n",
      "Epoch 208/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0195 - val_loss: 0.0287\n",
      "Epoch 209/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0194 - val_loss: 0.0288\n",
      "Epoch 210/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0194 - val_loss: 0.0289\n",
      "Epoch 211/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0193 - val_loss: 0.0290\n",
      "Epoch 212/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0192 - val_loss: 0.0292\n",
      "Epoch 213/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0192 - val_loss: 0.0293\n",
      "Epoch 214/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0191 - val_loss: 0.0294\n",
      "Epoch 215/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0190 - val_loss: 0.0295\n",
      "Epoch 216/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0190 - val_loss: 0.0296\n",
      "Epoch 217/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0189 - val_loss: 0.0298\n",
      "Epoch 218/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0188 - val_loss: 0.0299\n",
      "Epoch 219/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0188 - val_loss: 0.0300\n",
      "Epoch 220/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0187 - val_loss: 0.0301\n",
      "Epoch 221/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0186 - val_loss: 0.0303\n",
      "Epoch 222/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0186 - val_loss: 0.0304\n",
      "Epoch 223/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0185 - val_loss: 0.0305\n",
      "Epoch 224/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0185 - val_loss: 0.0306\n",
      "Epoch 225/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0184 - val_loss: 0.0308\n",
      "Epoch 226/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0183 - val_loss: 0.0309\n",
      "Epoch 227/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0183 - val_loss: 0.0310\n",
      "Epoch 228/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0182 - val_loss: 0.0312\n",
      "Epoch 229/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0182 - val_loss: 0.0313\n",
      "Epoch 230/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0181 - val_loss: 0.0314\n",
      "Epoch 231/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0181 - val_loss: 0.0315\n",
      "Epoch 232/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0180 - val_loss: 0.0317\n",
      "Epoch 233/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0180 - val_loss: 0.0318\n",
      "Epoch 234/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0179 - val_loss: 0.0319\n",
      "Epoch 235/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0179 - val_loss: 0.0321\n",
      "Epoch 236/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0178 - val_loss: 0.0322\n",
      "Epoch 237/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0178 - val_loss: 0.0323\n",
      "Epoch 238/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0177 - val_loss: 0.0324\n",
      "Epoch 239/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0177 - val_loss: 0.0326\n",
      "Epoch 240/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0176 - val_loss: 0.0327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0176 - val_loss: 0.0328\n",
      "Epoch 242/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0175 - val_loss: 0.0330\n",
      "Epoch 243/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0175 - val_loss: 0.0331\n",
      "Epoch 244/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0175 - val_loss: 0.0332\n",
      "Epoch 245/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0174 - val_loss: 0.0333\n",
      "Epoch 246/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0174 - val_loss: 0.0335\n",
      "Epoch 247/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0173 - val_loss: 0.0336\n",
      "Epoch 248/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0173 - val_loss: 0.0337\n",
      "Epoch 249/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0173 - val_loss: 0.0338\n",
      "Epoch 250/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0172 - val_loss: 0.0340\n",
      "Epoch 251/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0172 - val_loss: 0.0341\n",
      "Epoch 252/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0171 - val_loss: 0.0342\n",
      "Epoch 253/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0171 - val_loss: 0.0343\n",
      "Epoch 254/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0171 - val_loss: 0.0344\n",
      "Epoch 255/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0170 - val_loss: 0.0346\n",
      "Epoch 256/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0170 - val_loss: 0.0347\n",
      "Epoch 257/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0170 - val_loss: 0.0348\n",
      "Epoch 258/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0169 - val_loss: 0.0349\n",
      "Epoch 259/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0169 - val_loss: 0.0350\n",
      "Epoch 260/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0169 - val_loss: 0.0352\n",
      "Epoch 261/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0169 - val_loss: 0.0353\n",
      "Epoch 262/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0168 - val_loss: 0.0354\n",
      "Epoch 263/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0168 - val_loss: 0.0355\n",
      "Epoch 264/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0168 - val_loss: 0.0356\n",
      "Epoch 265/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0167 - val_loss: 0.0357\n",
      "Epoch 266/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0167 - val_loss: 0.0358\n",
      "Epoch 267/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0167 - val_loss: 0.0359\n",
      "Epoch 268/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0167 - val_loss: 0.0360\n",
      "Epoch 269/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0166 - val_loss: 0.0361\n",
      "Epoch 270/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0166 - val_loss: 0.0362\n",
      "Epoch 271/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0166 - val_loss: 0.0363\n",
      "Epoch 272/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0166 - val_loss: 0.0364\n",
      "Epoch 273/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0165 - val_loss: 0.0365\n",
      "Epoch 274/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0165 - val_loss: 0.0366\n",
      "Epoch 275/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0165 - val_loss: 0.0367\n",
      "Epoch 276/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0165 - val_loss: 0.0368\n",
      "Epoch 277/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0165 - val_loss: 0.0369\n",
      "Epoch 278/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0164 - val_loss: 0.0370\n",
      "Epoch 279/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0164 - val_loss: 0.0371\n",
      "Epoch 280/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0164 - val_loss: 0.0372\n",
      "Epoch 281/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0164 - val_loss: 0.0373\n",
      "Epoch 282/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0164 - val_loss: 0.0374\n",
      "Epoch 283/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0163 - val_loss: 0.0374\n",
      "Epoch 284/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0163 - val_loss: 0.0375\n",
      "Epoch 285/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0163 - val_loss: 0.0376\n",
      "Epoch 286/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0163 - val_loss: 0.0377\n",
      "Epoch 287/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0163 - val_loss: 0.0378\n",
      "Epoch 288/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0163 - val_loss: 0.0378\n",
      "Epoch 289/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0162 - val_loss: 0.0379\n",
      "Epoch 290/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0162 - val_loss: 0.0380\n",
      "Epoch 291/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0162 - val_loss: 0.0380\n",
      "Epoch 292/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0162 - val_loss: 0.0381\n",
      "Epoch 293/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0162 - val_loss: 0.0382\n",
      "Epoch 294/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0162 - val_loss: 0.0383\n",
      "Epoch 295/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0162 - val_loss: 0.0383\n",
      "Epoch 296/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0161 - val_loss: 0.0384\n",
      "Epoch 297/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0161 - val_loss: 0.0384\n",
      "Epoch 298/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0161 - val_loss: 0.0385\n",
      "Epoch 299/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0161 - val_loss: 0.0386\n",
      "Epoch 300/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0161 - val_loss: 0.0386\n",
      "Epoch 301/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0161 - val_loss: 0.0387\n",
      "Epoch 302/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0161 - val_loss: 0.0387\n",
      "Epoch 303/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0160 - val_loss: 0.0388\n",
      "Epoch 304/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0160 - val_loss: 0.0388\n",
      "Epoch 305/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0160 - val_loss: 0.0389\n",
      "Epoch 306/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0160 - val_loss: 0.0389\n",
      "Epoch 307/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0160 - val_loss: 0.0390\n",
      "Epoch 308/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0160 - val_loss: 0.0390\n",
      "Epoch 309/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0160 - val_loss: 0.0391\n",
      "Epoch 310/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0160 - val_loss: 0.0391\n",
      "Epoch 311/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0160 - val_loss: 0.0391\n",
      "Epoch 312/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0159 - val_loss: 0.0392\n",
      "Epoch 313/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0159 - val_loss: 0.0392\n",
      "Epoch 314/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0159 - val_loss: 0.0393\n",
      "Epoch 315/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0159 - val_loss: 0.0393\n",
      "Epoch 316/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0159 - val_loss: 0.0393\n",
      "Epoch 317/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0159 - val_loss: 0.0394\n",
      "Epoch 318/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0159 - val_loss: 0.0394\n",
      "Epoch 319/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0159 - val_loss: 0.0394\n",
      "Epoch 320/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0159 - val_loss: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0159 - val_loss: 0.0395\n",
      "Epoch 322/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0159 - val_loss: 0.0395\n",
      "Epoch 323/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0158 - val_loss: 0.0396\n",
      "Epoch 324/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0158 - val_loss: 0.0396\n",
      "Epoch 325/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0158 - val_loss: 0.0396\n",
      "Epoch 326/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0158 - val_loss: 0.0396\n",
      "Epoch 327/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0158 - val_loss: 0.0397\n",
      "Epoch 328/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0158 - val_loss: 0.0397\n",
      "Epoch 329/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0158 - val_loss: 0.0397\n",
      "Epoch 330/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0158 - val_loss: 0.0397\n",
      "Epoch 331/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0158 - val_loss: 0.0397\n",
      "Epoch 332/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0158 - val_loss: 0.0398\n",
      "Epoch 333/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0158 - val_loss: 0.0398\n",
      "Epoch 334/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0158 - val_loss: 0.0398\n",
      "Epoch 335/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0158 - val_loss: 0.0398\n",
      "Epoch 336/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0157 - val_loss: 0.0398\n",
      "Epoch 337/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0157 - val_loss: 0.0399\n",
      "Epoch 338/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0157 - val_loss: 0.0399\n",
      "Epoch 339/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0157 - val_loss: 0.0399\n",
      "Epoch 340/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0157 - val_loss: 0.0399\n",
      "Epoch 341/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0157 - val_loss: 0.0399\n",
      "Epoch 342/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0157 - val_loss: 0.0399\n",
      "Epoch 343/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0157 - val_loss: 0.0399\n",
      "Epoch 344/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 345/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 346/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 347/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 348/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 349/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 350/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 351/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 352/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0157 - val_loss: 0.0400\n",
      "Epoch 353/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 354/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 355/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 356/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 357/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 358/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 359/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 360/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 361/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 362/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 363/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 364/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 365/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 366/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 367/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0156 - val_loss: 0.0401\n",
      "Epoch 368/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 369/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 370/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 371/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 372/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 373/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 374/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 375/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0156 - val_loss: 0.0402\n",
      "Epoch 376/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 377/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 378/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 379/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 380/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 381/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 382/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 383/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 384/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 385/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 386/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 387/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 388/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 389/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 390/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 391/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 392/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 393/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 394/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 395/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 396/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 397/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 398/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0402\n",
      "Epoch 399/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 400/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 402/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 403/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 404/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 405/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 406/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 407/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 408/2000\n",
      "88/88 [==============================] - 0s 38us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 409/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 410/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0155 - val_loss: 0.0403\n",
      "Epoch 411/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 412/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 413/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 414/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 415/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 416/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 417/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 418/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 419/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 420/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 421/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 422/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 423/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 424/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 425/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 426/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 427/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 428/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 429/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 430/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 431/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 432/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 433/2000\n",
      "88/88 [==============================] - 0s 40us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 434/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 435/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 436/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 437/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 438/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 439/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 440/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 441/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 442/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 443/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 444/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 445/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 446/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 447/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 448/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 449/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 450/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 451/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 452/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 453/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 454/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 455/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 456/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 457/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 458/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 459/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 460/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 461/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 462/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 463/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 464/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 465/2000\n",
      "88/88 [==============================] - 0s 63us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 466/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 467/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 468/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 469/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 470/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 471/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 472/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 473/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 474/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 475/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 476/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 477/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 478/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 479/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 480/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 482/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 483/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 484/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 485/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 486/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 487/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 488/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 489/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0154 - val_loss: 0.0403\n",
      "Epoch 490/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 491/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 492/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 493/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 494/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 495/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 496/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 497/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 498/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 499/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 500/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 501/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 502/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 503/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 504/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 505/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 506/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 507/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 508/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 509/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 510/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 511/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 512/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 513/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 514/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 515/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 516/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 517/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 518/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 519/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 520/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 521/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 522/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 523/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 524/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 525/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 526/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 527/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 528/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 529/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 530/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 531/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 532/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 533/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 534/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 535/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 536/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 537/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 538/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 539/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 540/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0402\n",
      "Epoch 541/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 542/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 543/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 544/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 545/2000\n",
      "88/88 [==============================] - 0s 62us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 546/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 547/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 548/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 549/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 550/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 551/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 552/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 553/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 554/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 555/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 556/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 557/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 558/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 559/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 560/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 562/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 563/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 564/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 565/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 566/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 567/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 568/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 569/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 570/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 571/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 572/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 573/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 574/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 575/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 576/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 577/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 578/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 579/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 580/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 581/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 582/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 583/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 584/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 585/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 586/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0401\n",
      "Epoch 587/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 588/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 589/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 590/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 591/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 592/2000\n",
      "88/88 [==============================] - 0s 60us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 593/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 594/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 595/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 596/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 597/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 598/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 599/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 600/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 601/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 602/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 603/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 604/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 605/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 606/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 607/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 608/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 609/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 610/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 611/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 612/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 613/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 614/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 615/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 616/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 617/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 618/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 619/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 620/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 621/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 622/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 623/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 624/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 625/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 626/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 627/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 628/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 629/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 630/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 631/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 632/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 633/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 634/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 635/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 636/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 637/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 638/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0400\n",
      "Epoch 639/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 640/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 641/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 642/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 643/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 644/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 645/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 646/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 647/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 648/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 649/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 650/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 651/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 652/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 653/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 654/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 655/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 656/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 657/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 658/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 659/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 660/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 661/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 662/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 663/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 664/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 665/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 666/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 667/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 668/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 669/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 670/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 671/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 672/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 673/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 674/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 675/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 676/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 677/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 678/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 679/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 680/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 681/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 682/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 683/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 684/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 685/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 686/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 687/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 688/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 689/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 690/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 691/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 692/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 693/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 694/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 695/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 696/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 697/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 698/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 699/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 700/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 701/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 702/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 703/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 704/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 705/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 706/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 707/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 708/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 709/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 710/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 711/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 712/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 713/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 714/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 715/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 716/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 717/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 718/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 719/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 720/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 721/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 722/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 723/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 724/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 725/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 726/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 727/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 728/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 729/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 730/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 731/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 732/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 733/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 734/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 735/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 736/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 737/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 738/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 739/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 740/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 741/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 742/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 743/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 744/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 745/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 746/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 747/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 748/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 749/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 750/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 751/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 752/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 753/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 754/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 755/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 756/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 757/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 758/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 759/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 760/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 761/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 762/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 763/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 764/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 765/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 766/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 767/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 768/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 769/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 770/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 771/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 772/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 773/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 774/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 775/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 776/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 777/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 778/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 779/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 780/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 781/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 782/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 783/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 784/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 785/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 786/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 787/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 788/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 789/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 790/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 791/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 792/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 793/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 794/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 795/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 796/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 797/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 798/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 799/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 800/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 801/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 802/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 803/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 804/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 805/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 806/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 807/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 808/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 809/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 810/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 811/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 812/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 813/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 814/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 815/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 816/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 817/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 818/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 819/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 820/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 821/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 822/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 823/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 824/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 825/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 826/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 827/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 828/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 829/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 830/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 831/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 832/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 833/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 834/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 835/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 836/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 837/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 838/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 839/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 840/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 841/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 842/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 843/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 844/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 845/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 846/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 847/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 848/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 849/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 850/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 851/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 852/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 853/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 854/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 855/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 856/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 857/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 858/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 859/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 860/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 861/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 862/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 863/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 864/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 865/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 866/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 867/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 868/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 869/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 870/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 871/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 872/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0397\n",
      "Epoch 873/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 874/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 875/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 876/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 877/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 878/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 879/2000\n",
      "88/88 [==============================] - 0s 63us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 880/2000\n",
      "88/88 [==============================] - 0s 65us/step - loss: 0.0153 - val_loss: 0.0396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 881/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 882/2000\n",
      "88/88 [==============================] - 0s 60us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 883/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 884/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 885/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 886/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 887/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 888/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 889/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 890/2000\n",
      "88/88 [==============================] - 0s 40us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 891/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 892/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 893/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 894/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 895/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 896/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 897/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 898/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 899/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 900/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 901/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 902/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 903/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 904/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 905/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 906/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 907/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 908/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 909/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 910/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 911/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 912/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 913/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 914/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 915/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 916/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0153 - val_loss: 0.0396\n",
      "Epoch 917/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 918/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 919/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 920/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 921/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 922/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 923/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 924/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 925/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 926/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 927/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 928/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 929/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 930/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 931/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 932/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 933/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 934/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 935/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0396\n",
      "Epoch 936/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 937/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 938/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 939/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 940/2000\n",
      "88/88 [==============================] - 0s 62us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 941/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 942/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 943/2000\n",
      "88/88 [==============================] - 0s 40us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 944/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 945/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 946/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 947/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 948/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 949/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 950/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 951/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 952/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 953/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 954/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 955/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 956/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 957/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 958/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 959/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 960/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0152 - val_loss: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 961/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 962/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 963/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 964/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 965/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 966/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 967/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 968/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 969/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 970/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 971/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 972/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 973/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 974/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 975/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 976/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 977/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 978/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 979/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 980/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 981/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 982/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0395\n",
      "Epoch 983/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 984/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 985/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 986/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 987/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 988/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 989/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 990/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 991/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 992/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 993/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 994/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 995/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 996/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 997/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 998/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 999/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1000/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1001/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1002/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1003/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1004/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1005/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1006/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1007/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1008/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1009/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1010/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1011/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1012/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1013/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1014/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1015/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1016/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1017/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0394\n",
      "Epoch 1018/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1019/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1020/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1021/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1022/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1023/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1024/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1025/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1026/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1027/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1028/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1029/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1030/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1031/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1032/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1033/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1034/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1035/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1036/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1037/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1038/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1039/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1040/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1041/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1042/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1043/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1044/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0393\n",
      "Epoch 1045/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1046/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1047/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1048/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1049/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1050/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1051/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1052/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1053/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1054/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1055/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1056/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1057/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1058/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1059/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1060/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1061/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1062/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1063/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1064/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1065/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1066/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1067/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0152 - val_loss: 0.0392\n",
      "Epoch 1068/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1069/2000\n",
      "88/88 [==============================] - 0s 60us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1070/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1071/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1072/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1073/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1074/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1075/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1076/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1077/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1078/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1079/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1080/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1081/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0152 - val_loss: 0.0391\n",
      "Epoch 1082/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0151 - val_loss: 0.0391\n",
      "Epoch 1083/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0151 - val_loss: 0.0391\n",
      "Epoch 1084/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0151 - val_loss: 0.0391\n",
      "Epoch 1085/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0151 - val_loss: 0.0391\n",
      "Epoch 1086/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0151 - val_loss: 0.0391\n",
      "Epoch 1087/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1088/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1089/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1090/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1091/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1092/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1093/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1094/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1095/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1096/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1097/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1098/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1099/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1100/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1101/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1102/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1103/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0390\n",
      "Epoch 1104/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1105/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1106/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1107/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1108/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1109/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1110/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1111/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1112/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1113/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1114/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1115/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1116/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1117/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0151 - val_loss: 0.0389\n",
      "Epoch 1118/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1119/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1120/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1121/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1122/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1123/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1124/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1125/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1126/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1127/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1128/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1129/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1130/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0151 - val_loss: 0.0388\n",
      "Epoch 1131/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1132/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1133/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1134/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1135/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1136/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1137/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1138/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1139/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1140/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1141/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1142/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0151 - val_loss: 0.0387\n",
      "Epoch 1143/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0151 - val_loss: 0.0386\n",
      "Epoch 1144/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0151 - val_loss: 0.0386\n",
      "Epoch 1145/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0151 - val_loss: 0.0386\n",
      "Epoch 1146/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1147/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1148/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1149/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1150/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1151/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1152/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1153/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0150 - val_loss: 0.0386\n",
      "Epoch 1154/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1155/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1156/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1157/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1158/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1159/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1160/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1161/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1162/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0150 - val_loss: 0.0385\n",
      "Epoch 1163/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1164/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1165/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1166/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1167/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1168/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1169/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1170/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1171/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0150 - val_loss: 0.0384\n",
      "Epoch 1172/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1173/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1174/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1175/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1176/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1177/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1178/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1179/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1180/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0150 - val_loss: 0.0383\n",
      "Epoch 1181/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0150 - val_loss: 0.0382\n",
      "Epoch 1182/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0150 - val_loss: 0.0382\n",
      "Epoch 1183/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0150 - val_loss: 0.0382\n",
      "Epoch 1184/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0150 - val_loss: 0.0382\n",
      "Epoch 1185/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0149 - val_loss: 0.0382\n",
      "Epoch 1186/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0149 - val_loss: 0.0382\n",
      "Epoch 1187/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0149 - val_loss: 0.0382\n",
      "Epoch 1188/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0149 - val_loss: 0.0382\n",
      "Epoch 1189/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0149 - val_loss: 0.0381\n",
      "Epoch 1190/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0381\n",
      "Epoch 1191/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0149 - val_loss: 0.0381\n",
      "Epoch 1192/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0149 - val_loss: 0.0381\n",
      "Epoch 1193/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0149 - val_loss: 0.0381\n",
      "Epoch 1194/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0149 - val_loss: 0.0381\n",
      "Epoch 1195/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0381\n",
      "Epoch 1196/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0380\n",
      "Epoch 1197/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0149 - val_loss: 0.0380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1198/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0380\n",
      "Epoch 1199/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0380\n",
      "Epoch 1200/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0380\n",
      "Epoch 1201/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0380\n",
      "Epoch 1202/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0149 - val_loss: 0.0380\n",
      "Epoch 1203/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 1204/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 1205/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 1206/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 1207/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 1208/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 1209/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0149 - val_loss: 0.0379\n",
      "Epoch 1210/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0149 - val_loss: 0.0378\n",
      "Epoch 1211/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0149 - val_loss: 0.0378\n",
      "Epoch 1212/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0149 - val_loss: 0.0378\n",
      "Epoch 1213/2000\n",
      "88/88 [==============================] - 0s 65us/step - loss: 0.0149 - val_loss: 0.0378\n",
      "Epoch 1214/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0148 - val_loss: 0.0378\n",
      "Epoch 1215/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0148 - val_loss: 0.0378\n",
      "Epoch 1216/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0148 - val_loss: 0.0378\n",
      "Epoch 1217/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0148 - val_loss: 0.0377\n",
      "Epoch 1218/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0148 - val_loss: 0.0377\n",
      "Epoch 1219/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0148 - val_loss: 0.0377\n",
      "Epoch 1220/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0148 - val_loss: 0.0377\n",
      "Epoch 1221/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0148 - val_loss: 0.0377\n",
      "Epoch 1222/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0148 - val_loss: 0.0377\n",
      "Epoch 1223/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0148 - val_loss: 0.0376\n",
      "Epoch 1224/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0148 - val_loss: 0.0376\n",
      "Epoch 1225/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0148 - val_loss: 0.0376\n",
      "Epoch 1226/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0148 - val_loss: 0.0376\n",
      "Epoch 1227/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0148 - val_loss: 0.0376\n",
      "Epoch 1228/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0148 - val_loss: 0.0376\n",
      "Epoch 1229/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0148 - val_loss: 0.0375\n",
      "Epoch 1230/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0148 - val_loss: 0.0375\n",
      "Epoch 1231/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0148 - val_loss: 0.0375\n",
      "Epoch 1232/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0148 - val_loss: 0.0375\n",
      "Epoch 1233/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0148 - val_loss: 0.0375\n",
      "Epoch 1234/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0148 - val_loss: 0.0375\n",
      "Epoch 1235/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0148 - val_loss: 0.0374\n",
      "Epoch 1236/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0148 - val_loss: 0.0374\n",
      "Epoch 1237/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0147 - val_loss: 0.0374\n",
      "Epoch 1238/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0147 - val_loss: 0.0374\n",
      "Epoch 1239/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0147 - val_loss: 0.0374\n",
      "Epoch 1240/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0147 - val_loss: 0.0373\n",
      "Epoch 1241/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0147 - val_loss: 0.0373\n",
      "Epoch 1242/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0147 - val_loss: 0.0373\n",
      "Epoch 1243/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0147 - val_loss: 0.0373\n",
      "Epoch 1244/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0147 - val_loss: 0.0373\n",
      "Epoch 1245/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0147 - val_loss: 0.0373\n",
      "Epoch 1246/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0147 - val_loss: 0.0372\n",
      "Epoch 1247/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0147 - val_loss: 0.0372\n",
      "Epoch 1248/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0147 - val_loss: 0.0372\n",
      "Epoch 1249/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0147 - val_loss: 0.0372\n",
      "Epoch 1250/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0147 - val_loss: 0.0372\n",
      "Epoch 1251/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0147 - val_loss: 0.0371\n",
      "Epoch 1252/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0147 - val_loss: 0.0371\n",
      "Epoch 1253/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0147 - val_loss: 0.0371\n",
      "Epoch 1254/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0147 - val_loss: 0.0371\n",
      "Epoch 1255/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0147 - val_loss: 0.0371\n",
      "Epoch 1256/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0147 - val_loss: 0.0370\n",
      "Epoch 1257/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0146 - val_loss: 0.0370\n",
      "Epoch 1258/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0146 - val_loss: 0.0370\n",
      "Epoch 1259/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0146 - val_loss: 0.0370\n",
      "Epoch 1260/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0146 - val_loss: 0.0370\n",
      "Epoch 1261/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0146 - val_loss: 0.0369\n",
      "Epoch 1262/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0146 - val_loss: 0.0369\n",
      "Epoch 1263/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0146 - val_loss: 0.0369\n",
      "Epoch 1264/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0146 - val_loss: 0.0369\n",
      "Epoch 1265/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0146 - val_loss: 0.0369\n",
      "Epoch 1266/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0146 - val_loss: 0.0368\n",
      "Epoch 1267/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0146 - val_loss: 0.0368\n",
      "Epoch 1268/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0146 - val_loss: 0.0368\n",
      "Epoch 1269/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0146 - val_loss: 0.0368\n",
      "Epoch 1270/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0146 - val_loss: 0.0368\n",
      "Epoch 1271/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0146 - val_loss: 0.0367\n",
      "Epoch 1272/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0146 - val_loss: 0.0367\n",
      "Epoch 1273/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0146 - val_loss: 0.0367\n",
      "Epoch 1274/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0146 - val_loss: 0.0367\n",
      "Epoch 1275/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0146 - val_loss: 0.0367\n",
      "Epoch 1276/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0145 - val_loss: 0.0366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1277/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0145 - val_loss: 0.0366\n",
      "Epoch 1278/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0145 - val_loss: 0.0366\n",
      "Epoch 1279/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0145 - val_loss: 0.0366\n",
      "Epoch 1280/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0145 - val_loss: 0.0366\n",
      "Epoch 1281/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0145 - val_loss: 0.0365\n",
      "Epoch 1282/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0145 - val_loss: 0.0365\n",
      "Epoch 1283/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0145 - val_loss: 0.0365\n",
      "Epoch 1284/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0145 - val_loss: 0.0365\n",
      "Epoch 1285/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0145 - val_loss: 0.0364\n",
      "Epoch 1286/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0145 - val_loss: 0.0364\n",
      "Epoch 1287/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0145 - val_loss: 0.0364\n",
      "Epoch 1288/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0145 - val_loss: 0.0364\n",
      "Epoch 1289/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0145 - val_loss: 0.0364\n",
      "Epoch 1290/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0145 - val_loss: 0.0363\n",
      "Epoch 1291/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0145 - val_loss: 0.0363\n",
      "Epoch 1292/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0145 - val_loss: 0.0363\n",
      "Epoch 1293/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0145 - val_loss: 0.0363\n",
      "Epoch 1294/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0144 - val_loss: 0.0363\n",
      "Epoch 1295/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0144 - val_loss: 0.0362\n",
      "Epoch 1296/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0144 - val_loss: 0.0362\n",
      "Epoch 1297/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0144 - val_loss: 0.0362\n",
      "Epoch 1298/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0144 - val_loss: 0.0362\n",
      "Epoch 1299/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0144 - val_loss: 0.0361\n",
      "Epoch 1300/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0144 - val_loss: 0.0361\n",
      "Epoch 1301/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0144 - val_loss: 0.0361\n",
      "Epoch 1302/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0144 - val_loss: 0.0361\n",
      "Epoch 1303/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0144 - val_loss: 0.0361\n",
      "Epoch 1304/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0144 - val_loss: 0.0360\n",
      "Epoch 1305/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0144 - val_loss: 0.0360\n",
      "Epoch 1306/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0144 - val_loss: 0.0360\n",
      "Epoch 1307/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0144 - val_loss: 0.0360\n",
      "Epoch 1308/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0144 - val_loss: 0.0360\n",
      "Epoch 1309/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0144 - val_loss: 0.0359\n",
      "Epoch 1310/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0144 - val_loss: 0.0359\n",
      "Epoch 1311/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0144 - val_loss: 0.0359\n",
      "Epoch 1312/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0143 - val_loss: 0.0359\n",
      "Epoch 1313/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0143 - val_loss: 0.0358\n",
      "Epoch 1314/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0143 - val_loss: 0.0358\n",
      "Epoch 1315/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0143 - val_loss: 0.0358\n",
      "Epoch 1316/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0143 - val_loss: 0.0358\n",
      "Epoch 1317/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0143 - val_loss: 0.0358\n",
      "Epoch 1318/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0143 - val_loss: 0.0357\n",
      "Epoch 1319/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0143 - val_loss: 0.0357\n",
      "Epoch 1320/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0143 - val_loss: 0.0357\n",
      "Epoch 1321/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0143 - val_loss: 0.0357\n",
      "Epoch 1322/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0143 - val_loss: 0.0357\n",
      "Epoch 1323/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0143 - val_loss: 0.0356\n",
      "Epoch 1324/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0143 - val_loss: 0.0356\n",
      "Epoch 1325/2000\n",
      "88/88 [==============================] - 0s 41us/step - loss: 0.0143 - val_loss: 0.0356\n",
      "Epoch 1326/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0143 - val_loss: 0.0356\n",
      "Epoch 1327/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0143 - val_loss: 0.0356\n",
      "Epoch 1328/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0143 - val_loss: 0.0355\n",
      "Epoch 1329/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0143 - val_loss: 0.0355\n",
      "Epoch 1330/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0143 - val_loss: 0.0355\n",
      "Epoch 1331/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0142 - val_loss: 0.0355\n",
      "Epoch 1332/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0142 - val_loss: 0.0355\n",
      "Epoch 1333/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0142 - val_loss: 0.0354\n",
      "Epoch 1334/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0142 - val_loss: 0.0354\n",
      "Epoch 1335/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0142 - val_loss: 0.0354\n",
      "Epoch 1336/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0142 - val_loss: 0.0354\n",
      "Epoch 1337/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0142 - val_loss: 0.0353\n",
      "Epoch 1338/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0142 - val_loss: 0.0353\n",
      "Epoch 1339/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0142 - val_loss: 0.0353\n",
      "Epoch 1340/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0142 - val_loss: 0.0353\n",
      "Epoch 1341/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0142 - val_loss: 0.0353\n",
      "Epoch 1342/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 1343/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 1344/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 1345/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 1346/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 1347/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 1348/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0142 - val_loss: 0.0351\n",
      "Epoch 1349/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0142 - val_loss: 0.0351\n",
      "Epoch 1350/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0142 - val_loss: 0.0351\n",
      "Epoch 1351/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0142 - val_loss: 0.0351\n",
      "Epoch 1352/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0141 - val_loss: 0.0351\n",
      "Epoch 1353/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0141 - val_loss: 0.0350\n",
      "Epoch 1354/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0141 - val_loss: 0.0350\n",
      "Epoch 1355/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0141 - val_loss: 0.0350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1356/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0350\n",
      "Epoch 1357/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0141 - val_loss: 0.0350\n",
      "Epoch 1358/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0349\n",
      "Epoch 1359/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0141 - val_loss: 0.0349\n",
      "Epoch 1360/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0349\n",
      "Epoch 1361/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0349\n",
      "Epoch 1362/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0141 - val_loss: 0.0349\n",
      "Epoch 1363/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0141 - val_loss: 0.0349\n",
      "Epoch 1364/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0348\n",
      "Epoch 1365/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0141 - val_loss: 0.0348\n",
      "Epoch 1366/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0348\n",
      "Epoch 1367/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0141 - val_loss: 0.0348\n",
      "Epoch 1368/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0348\n",
      "Epoch 1369/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0141 - val_loss: 0.0347\n",
      "Epoch 1370/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0141 - val_loss: 0.0347\n",
      "Epoch 1371/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0141 - val_loss: 0.0347\n",
      "Epoch 1372/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0141 - val_loss: 0.0347\n",
      "Epoch 1373/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0141 - val_loss: 0.0347\n",
      "Epoch 1374/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0141 - val_loss: 0.0347\n",
      "Epoch 1375/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0141 - val_loss: 0.0346\n",
      "Epoch 1376/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0141 - val_loss: 0.0346\n",
      "Epoch 1377/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0346\n",
      "Epoch 1378/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0346\n",
      "Epoch 1379/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0140 - val_loss: 0.0346\n",
      "Epoch 1380/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0140 - val_loss: 0.0346\n",
      "Epoch 1381/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0346\n",
      "Epoch 1382/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0140 - val_loss: 0.0345\n",
      "Epoch 1383/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0140 - val_loss: 0.0345\n",
      "Epoch 1384/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0140 - val_loss: 0.0345\n",
      "Epoch 1385/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0345\n",
      "Epoch 1386/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0140 - val_loss: 0.0345\n",
      "Epoch 1387/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0345\n",
      "Epoch 1388/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "Epoch 1389/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "Epoch 1390/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "Epoch 1391/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "Epoch 1392/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "Epoch 1393/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "Epoch 1394/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0140 - val_loss: 0.0344\n",
      "Epoch 1395/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0140 - val_loss: 0.0343\n",
      "Epoch 1396/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0140 - val_loss: 0.0343\n",
      "Epoch 1397/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0140 - val_loss: 0.0343\n",
      "Epoch 1398/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0140 - val_loss: 0.0343\n",
      "Epoch 1399/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0140 - val_loss: 0.0343\n",
      "Epoch 1400/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0140 - val_loss: 0.0343\n",
      "Epoch 1401/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0140 - val_loss: 0.0343\n",
      "Epoch 1402/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1403/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1404/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1405/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1406/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1407/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1408/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1409/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0140 - val_loss: 0.0342\n",
      "Epoch 1410/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0140 - val_loss: 0.0341\n",
      "Epoch 1411/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1412/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1413/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1414/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1415/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1416/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1417/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1418/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0341\n",
      "Epoch 1419/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1420/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1421/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1422/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1423/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1424/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1425/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1426/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1427/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1428/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0139 - val_loss: 0.0340\n",
      "Epoch 1429/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1430/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1431/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1432/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1433/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1434/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1435/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1436/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1437/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1438/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1439/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0339\n",
      "Epoch 1440/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1441/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1442/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1443/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1444/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1445/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1446/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1447/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1448/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1449/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1450/2000\n",
      "88/88 [==============================] - 0s 60us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1451/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1452/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0139 - val_loss: 0.0338\n",
      "Epoch 1453/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1454/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1455/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1456/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1457/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1458/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1459/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1460/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1461/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1462/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1463/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1464/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1465/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1466/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1467/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1468/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0139 - val_loss: 0.0337\n",
      "Epoch 1469/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1470/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1471/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1472/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1473/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1474/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1475/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1476/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1477/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1478/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1479/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1480/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1481/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1482/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1483/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1484/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1485/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1486/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1487/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1488/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1489/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1490/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1491/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0336\n",
      "Epoch 1492/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1493/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1494/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1495/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1496/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1497/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1498/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1499/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1500/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1501/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1502/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1503/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1504/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1505/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1506/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1507/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1508/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1509/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1510/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1511/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1512/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1513/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1514/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1515/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1516/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1517/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1518/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1519/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1520/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1521/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1522/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1523/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1524/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1525/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1526/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1527/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1528/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1529/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1530/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1531/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1532/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1533/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1534/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1535/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1536/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1537/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1538/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1539/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1540/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1541/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1542/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1543/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1544/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0335\n",
      "Epoch 1545/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1546/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1547/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1548/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1549/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1550/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1551/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1552/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1553/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1554/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1555/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1556/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1557/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1558/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1559/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1560/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1561/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1562/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1563/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1564/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1565/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1566/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1567/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1568/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1569/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1570/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1571/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1572/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1573/2000\n",
      "88/88 [==============================] - 0s 65us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1574/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0138 - val_loss: 0.0334\n",
      "Epoch 1575/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1576/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1577/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1578/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1579/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1580/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1581/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1582/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1583/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1584/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1585/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1586/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1587/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1588/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1589/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1590/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1591/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0334\n",
      "Epoch 1592/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1593/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1594/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1595/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1596/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1597/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1598/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1599/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1600/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1601/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1602/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1603/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1604/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1605/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1606/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1607/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1608/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1609/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1610/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1611/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1612/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1613/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1614/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1615/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1616/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1617/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1618/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1619/2000\n",
      "88/88 [==============================] - 0s 60us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1620/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1621/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1622/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1623/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1624/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1625/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1626/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1627/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1628/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1629/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1630/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1631/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1632/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1633/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1634/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1635/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1636/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1637/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1638/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1639/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1640/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1641/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1642/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1643/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1644/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1645/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1646/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1647/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1648/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1649/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1650/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1651/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1652/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1653/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1654/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1655/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1656/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1657/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1658/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1659/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1660/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1661/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1662/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1663/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1664/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1665/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1666/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1667/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1668/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1669/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1670/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1671/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1672/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1673/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1674/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1675/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1676/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1677/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1678/2000\n",
      "88/88 [==============================] - 0s 40us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1679/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1680/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1681/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1682/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1683/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1684/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0137 - val_loss: 0.0335\n",
      "Epoch 1685/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1686/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1687/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1688/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1689/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1690/2000\n",
      "88/88 [==============================] - 0s 59us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1691/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1692/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0136 - val_loss: 0.0335\n",
      "Epoch 1693/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1694/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1695/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1696/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1697/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1698/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1699/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1700/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1701/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1702/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1703/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1704/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1705/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1706/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1707/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1708/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1709/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1710/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1711/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1712/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1713/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1714/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1715/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1716/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1717/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1718/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1719/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1720/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1721/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1722/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1723/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1724/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1725/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1726/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1727/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1728/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1729/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1730/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1731/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1732/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1733/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1734/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1735/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1736/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1737/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1738/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1739/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1740/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1741/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1742/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1743/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1744/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1745/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1746/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1747/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1748/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1749/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1750/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1751/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1752/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1753/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1754/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1755/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1756/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1757/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1758/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1759/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1760/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1761/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1762/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1763/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1764/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1765/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1766/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0336\n",
      "Epoch 1767/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1768/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1769/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1770/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1771/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1772/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1773/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1774/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0136 - val_loss: 0.0337\n",
      "Epoch 1775/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1776/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1777/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1778/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1779/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1780/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1781/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1782/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1783/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1784/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1785/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1786/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1787/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1788/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1789/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1790/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1791/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1792/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1793/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1794/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1795/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1796/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1797/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1798/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1799/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1800/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1801/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1802/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1803/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1804/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1805/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1806/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1807/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1808/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1809/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1810/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1811/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1812/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1813/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1814/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1815/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1816/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1817/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1818/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1819/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1820/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1821/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1822/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1823/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1824/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1825/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1826/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1827/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1828/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0337\n",
      "Epoch 1829/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1830/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1831/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1832/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1833/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1834/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1835/2000\n",
      "88/88 [==============================] - 0s 42us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1836/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1837/2000\n",
      "88/88 [==============================] - 0s 43us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1838/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1839/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1840/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1841/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1842/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1843/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1844/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1845/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1846/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1847/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1848/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0135 - val_loss: 0.0338\n",
      "Epoch 1849/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1850/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1851/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1852/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1853/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1854/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1855/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1856/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1857/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1858/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1859/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1860/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1861/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1862/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1863/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1864/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1865/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1866/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1867/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1868/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1869/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1870/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1871/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1872/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1873/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1874/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1875/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1876/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1877/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0338\n",
      "Epoch 1878/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1879/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1880/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1881/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1882/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1883/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1884/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1885/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1886/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1887/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1888/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1889/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1890/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1891/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1892/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1893/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1894/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1895/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1896/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1897/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1898/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1899/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1900/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1901/2000\n",
      "88/88 [==============================] - 0s 61us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1902/2000\n",
      "88/88 [==============================] - 0s 55us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1903/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1904/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1905/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1906/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1907/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1908/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1909/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1910/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1911/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1912/2000\n",
      "88/88 [==============================] - 0s 58us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1913/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1914/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0134 - val_loss: 0.0339\n",
      "Epoch 1915/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0339\n",
      "Epoch 1916/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0339\n",
      "Epoch 1917/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1918/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1919/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1920/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1921/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1922/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1923/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1924/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1925/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1926/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1927/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1928/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1929/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1930/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1931/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1932/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1933/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1934/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1935/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1936/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1937/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1938/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1939/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1940/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1941/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1942/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1943/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1944/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1945/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1946/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1947/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1948/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0133 - val_loss: 0.0340\n",
      "Epoch 1949/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1950/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1951/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1952/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1953/2000\n",
      "88/88 [==============================] - 0s 57us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1954/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1955/2000\n",
      "88/88 [==============================] - 0s 54us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1956/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1957/2000\n",
      "88/88 [==============================] - 0s 56us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1958/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1959/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1960/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1961/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1962/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1963/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1964/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1965/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1966/2000\n",
      "88/88 [==============================] - 0s 53us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1967/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1968/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1969/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1970/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1971/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1972/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1973/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1974/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0133 - val_loss: 0.0341\n",
      "Epoch 1975/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0133 - val_loss: 0.0342\n",
      "Epoch 1976/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1977/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1978/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1979/2000\n",
      "88/88 [==============================] - 0s 52us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1980/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1981/2000\n",
      "88/88 [==============================] - 0s 51us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1982/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1983/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1984/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1985/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1986/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1987/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0132 - val_loss: 0.0342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1988/2000\n",
      "88/88 [==============================] - 0s 50us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1989/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1990/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1991/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1992/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1993/2000\n",
      "88/88 [==============================] - 0s 49us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1994/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1995/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1996/2000\n",
      "88/88 [==============================] - 0s 46us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1997/2000\n",
      "88/88 [==============================] - 0s 47us/step - loss: 0.0132 - val_loss: 0.0342\n",
      "Epoch 1998/2000\n",
      "88/88 [==============================] - 0s 45us/step - loss: 0.0132 - val_loss: 0.0343\n",
      "Epoch 1999/2000\n",
      "88/88 [==============================] - 0s 48us/step - loss: 0.0132 - val_loss: 0.0343\n",
      "Epoch 2000/2000\n",
      "88/88 [==============================] - 0s 44us/step - loss: 0.0132 - val_loss: 0.0343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18f8852080>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(new_train_X,new_train_Y, nb_epoch=2000, batch_size=512, validation_split = .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:100]['lynx']\n",
    "test = list(df[100:]['lynx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[229, 399, 1132, 2432, 3574, 2935, 1537, 529, 485, 662, 1000, 1590, 2657, 3396]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 203111.355\n"
     ]
    }
   ],
   "source": [
    "test_extended = train.tolist()[-1*window_size:] + test\n",
    "test_data = []\n",
    "for i in test_extended:\n",
    "    try:\n",
    "        test_data.append(i[0])\n",
    "    except:\n",
    "        test_data.append(i)\n",
    "test_data = np.array(test_data).reshape(-1,1)\n",
    "test_scaled = min_max_scaler.fit_transform(test_data)\n",
    "test_X,test_Y = [],[]\n",
    "for i in range(0 , len(test_scaled) - window_size):\n",
    "    test_X.append(test_scaled[i:i+window_size])\n",
    "    test_Y.append(test_scaled[i+window_size])\n",
    "    new_test_X,new_test_Y = [],[]\n",
    "for i in test_X:\n",
    "    new_test_X.append(i.reshape(-1))\n",
    "for i in test_Y:\n",
    "    new_test_Y.append(i.reshape(-1))\n",
    "new_test_X = np.array(new_test_X)\n",
    "new_test_Y = np.array(new_test_Y)\n",
    "#new_test_X = np.reshape(new_test_X, (new_test_X.shape[0], new_test_X.shape[1], 1))\n",
    "predictions = model.predict(new_test_X)\n",
    "predictions_rescaled=min_max_scaler.inverse_transform(predictions)\n",
    "Y = pd.DataFrame(test)\n",
    "pred = pd.DataFrame(predictions_rescaled)\n",
    "plt.plot(Y,label = 'Real')\n",
    "plt.plot(pred , color = 'r', label = 'Pred_ANN')\n",
    "#p.plot()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "error = mse(test,predictions_rescaled)\n",
    "print('Test MSE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.015\n"
     ]
    }
   ],
   "source": [
    "plt.plot(test_scaled[-14:],label = 'Real')\n",
    "plt.plot(predictions, color = 'r', label = 'Pred_ANN')\n",
    "#p.plot()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "error = mse(test_scaled[-14:], predictions)\n",
    "print('Test MSE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('lynx_train_X.csv', new_train_X)\n",
    "np.savetxt('lynx_train_Y.csv', new_train_Y)\n",
    "np.savetxt('lynx_test_X.csv', new_test_X)\n",
    "np.savetxt('lynx_predictions_ann.csv', predictions)\n",
    "np.savetxt('lynx_test_Y.csv', test_scaled[-14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
